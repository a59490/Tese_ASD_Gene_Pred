+ echo 'Now on: /work/joaoinacio/work/Tese_ASD_Gene_Pred'
Now on: /work/joaoinacio/work/Tese_ASD_Gene_Pred
+ /work/joaoinacio/miniconda3/bin/conda run -n dna python /work/joaoinacio/work/Tese_ASD_Gene_Pred/Embeddings.py
Downloading pytorch_model.bin:   0%|          | 0.00/468M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|▏         | 10.5M/468M [00:00<00:09, 47.3MB/s]Downloading pytorch_model.bin:   4%|▍         | 21.0M/468M [00:00<00:07, 61.4MB/s]Downloading pytorch_model.bin:   7%|▋         | 31.5M/468M [00:00<00:05, 74.4MB/s]Downloading pytorch_model.bin:  11%|█         | 52.4M/468M [00:00<00:04, 100MB/s] Downloading pytorch_model.bin:  16%|█▌        | 73.4M/468M [00:00<00:03, 120MB/s]Downloading pytorch_model.bin:  20%|██        | 94.4M/468M [00:00<00:02, 140MB/s]Downloading pytorch_model.bin:  25%|██▍       | 115M/468M [00:00<00:02, 153MB/s] Downloading pytorch_model.bin:  29%|██▉       | 136M/468M [00:01<00:02, 160MB/s]Downloading pytorch_model.bin:  34%|███▎      | 157M/468M [00:01<00:01, 170MB/s]Downloading pytorch_model.bin:  38%|███▊      | 178M/468M [00:01<00:01, 170MB/s]Downloading pytorch_model.bin:  43%|████▎     | 199M/468M [00:01<00:01, 174MB/s]Downloading pytorch_model.bin:  47%|████▋     | 220M/468M [00:01<00:01, 178MB/s]Downloading pytorch_model.bin:  51%|█████▏    | 241M/468M [00:01<00:01, 180MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 262M/468M [00:01<00:01, 181MB/s]Downloading pytorch_model.bin:  60%|██████    | 283M/468M [00:01<00:01, 179MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 304M/468M [00:02<00:00, 180MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 325M/468M [00:02<00:00, 178MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 346M/468M [00:02<00:00, 178MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 367M/468M [00:02<00:00, 180MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 388M/468M [00:02<00:00, 181MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 409M/468M [00:02<00:00, 175MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 430M/468M [00:02<00:00, 174MB/s]Downloading pytorch_model.bin:  96%|█████████▋| 451M/468M [00:02<00:00, 179MB/s]Downloading pytorch_model.bin: 100%|██████████| 468M/468M [00:02<00:00, 159MB/s]
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 512 to 613
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 613 to 730
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 730 to 879
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 879 to 1014
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1014 to 1616
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1616 to 1679
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1679 to 1927
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1927 to 1935
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1935 to 1939
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1939 to 1949
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1949 to 1956
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 1956 to 2004
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 2004 to 2016
  warnings.warn(
/work/joaoinacio/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/1d020b803b871a976f5f3d5565f0eac8f2c7bb81/bert_layers.py:432: UserWarning: Increasing alibi size from 2016 to 2095
  warnings.warn(

+ echo Finished
Finished
